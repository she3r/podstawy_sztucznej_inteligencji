{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "print(tf.__version__)\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, SimpleRNN, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "It must specify 3 arguments:\n",
    "\n",
    "* **input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "* **output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "* **input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "\n",
    "# Zad.\n",
    "\n",
    "Pobierz dane z konkursu http://2017.poleval.pl/index.php/tasks/ Task 2. Sentiment analysis\n",
    "\n",
    "Pojedyńczy plik można pobrać z tąd:\n",
    "https://www.dropbox.com/sh/tjq47ybybgnrbel/AAAVbp0UkQTAbKWVMIi5mtHpa?dl=0\n",
    "\n",
    "## Zad\n",
    "Zbuduj model na podstawie warstwy Embedding, która się uczy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n"
     ]
    }
   ],
   "source": [
    "file_with_filtered_embeddings = \"Dane/data_poleval/embeddings.txt\"\n",
    "\n",
    "words2ids = {}\n",
    "embeddings = []\n",
    "\n",
    "embeddings.append(np.zeros(300)) # rezerwujemy embeddingi na paddin i nieznane slowa\n",
    "embeddings.append(np.zeros(300))\n",
    "\n",
    "i = 0\n",
    "with open(file_with_filtered_embeddings,\"r\", encoding=\"utf8\")as f:\n",
    "    for line in f:\n",
    "        toks = line.split(\" \")\n",
    "        word = toks[0]\n",
    "        embeddings.append(np.array([float(x) for x in toks[1:]]))\n",
    "        words2ids[word] = i+2 # +3 - przesuniecie po to zeby specjalne embeddingi byly na pozycji 0 i 1\n",
    "        i = i + 1\n",
    "\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2731"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2ids[\"Słodkawy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0695675 , -0.0123177 ,  0.0190564 ,  0.0205853 , -0.0586069 ,\n",
       "        0.123962  , -0.0113998 ,  0.0486168 , -0.0593128 ,  0.0244056 ,\n",
       "        0.00806909, -0.0123139 ,  0.0318254 ,  0.0375928 ,  0.0488764 ,\n",
       "       -0.0311151 , -0.0571845 ,  0.00886492, -0.023981  , -0.0729585 ,\n",
       "        0.00780728, -0.0103228 ,  0.0451543 , -0.00375595, -0.01016   ,\n",
       "        0.0157066 ,  0.0933215 ,  0.00742586,  0.0662036 ,  0.0147866 ,\n",
       "       -0.0326982 , -0.105277  ,  0.0939024 , -0.141137  , -0.0390246 ,\n",
       "        0.0307554 , -0.124878  ,  0.092467  , -0.0449775 ,  0.0278756 ,\n",
       "        0.0120974 ,  0.0402319 , -0.0602183 ,  0.117348  , -0.0286395 ,\n",
       "        0.0226578 , -0.0100841 ,  0.0928551 ,  0.0676152 , -0.0280563 ,\n",
       "        0.0516412 ,  0.0899037 , -0.0634045 ,  0.0149937 , -0.0472655 ,\n",
       "       -0.0638853 , -0.00977132, -0.0440674 ,  0.00945409, -0.0873957 ,\n",
       "        0.0113474 ,  0.00536351,  0.0406797 ,  0.051127  ,  0.0194782 ,\n",
       "        0.00793247,  0.00938793,  0.0813627 , -0.0323478 ,  0.00834836,\n",
       "       -0.0307443 , -0.00129854,  0.0432319 ,  0.133133  ,  0.025146  ,\n",
       "       -0.00617633, -0.0998303 ,  0.0437879 ,  0.139863  ,  0.0770133 ,\n",
       "        0.00529203, -0.0904055 ,  0.0356017 , -0.092935  ,  0.00502568,\n",
       "        0.00720983,  0.0144587 ,  0.0504662 ,  0.106105  , -0.0596305 ,\n",
       "        0.0165428 , -0.0756499 , -0.0548418 , -0.0436699 , -0.00525561,\n",
       "        0.0554075 ,  0.0690609 ,  0.09896   ,  0.0120319 , -0.0420232 ,\n",
       "        0.00035797, -0.0137097 ,  0.0175877 , -0.148398  , -0.0668958 ,\n",
       "        0.0681549 , -0.0670223 , -0.00222621, -0.047205  , -0.0354883 ,\n",
       "        0.0521945 , -0.00938699, -0.110972  , -0.0411835 , -0.0262399 ,\n",
       "       -0.0245242 , -0.0205056 ,  0.119049  , -0.0968168 ,  0.00101308,\n",
       "        0.0438422 ,  0.0132003 , -0.0552679 ,  0.0358181 ,  0.0659297 ,\n",
       "       -0.0691131 ,  0.0380908 ,  0.0678329 , -0.0180567 , -0.0153763 ,\n",
       "       -0.0928159 ,  0.0291021 ,  0.0661943 , -0.0232954 , -0.0999478 ,\n",
       "       -0.025268  ,  0.0265694 , -0.0789217 , -0.139136  , -0.043029  ,\n",
       "       -0.004668  ,  0.142589  ,  0.0491691 ,  0.0301554 , -0.0725976 ,\n",
       "       -0.0245116 , -0.0258727 , -0.081907  , -0.033818  , -0.0910337 ,\n",
       "        0.134159  , -0.0114324 ,  0.168495  ,  0.0744587 , -0.0551564 ,\n",
       "       -0.0299129 , -0.0764537 , -0.0583725 ,  0.101149  ,  0.0483574 ,\n",
       "        0.0185893 ,  0.0476507 , -0.00851304,  0.0257832 , -0.0423777 ,\n",
       "       -0.026234  , -0.0224656 , -0.0177372 , -0.0269845 , -0.0395937 ,\n",
       "       -0.0895522 , -0.0387699 ,  0.0386046 , -0.0269241 , -0.0252017 ,\n",
       "        0.00555369, -0.125734  ,  0.052703  , -0.0508996 , -0.0301596 ,\n",
       "        0.0719947 , -0.00781578,  0.0183722 ,  0.0137835 ,  0.145643  ,\n",
       "       -0.163511  ,  0.00493691,  0.00842864, -0.00523769,  0.0250127 ,\n",
       "       -0.069887  ,  0.0340264 , -0.0277308 ,  0.0666341 ,  0.0452207 ,\n",
       "        0.00394535,  0.159992  , -0.0478898 , -0.0356679 , -0.0167176 ,\n",
       "        0.0389694 ,  0.0144782 , -0.0481584 , -0.0269981 ,  0.0665881 ,\n",
       "       -0.00597176,  0.0203096 , -0.0116472 , -0.0288414 , -0.0123771 ,\n",
       "       -0.0126907 ,  0.0264299 , -0.094251  ,  0.0308239 , -0.0451183 ,\n",
       "       -0.00945021, -0.073934  ,  0.0713121 , -0.0174938 ,  0.0587443 ,\n",
       "       -0.00283306, -0.0534264 ,  0.0672013 , -0.0402565 , -0.0248949 ,\n",
       "       -0.0521673 , -0.00848431,  0.0723062 , -0.00412129,  0.0725852 ,\n",
       "        0.068625  ,  0.00367457, -0.0591261 , -0.113534  ,  0.0148794 ,\n",
       "       -0.0679527 ,  0.0171171 , -0.0326562 ,  0.087949  , -0.0723946 ,\n",
       "       -0.0454645 , -0.0426597 ,  0.0783788 ,  0.0905034 ,  0.05728   ,\n",
       "       -0.0858484 , -0.064867  ,  0.0215164 ,  0.0423614 , -0.0340275 ,\n",
       "        0.0465895 ,  0.0276519 ,  0.0410746 ,  0.0158002 , -0.154203  ,\n",
       "        0.0731088 ,  0.0101519 , -0.0274256 , -0.0583854 , -0.0184802 ,\n",
       "       -0.0752445 ,  0.0297846 ,  0.0426677 , -0.0593996 ,  0.0168314 ,\n",
       "        0.0689239 , -0.00937583, -0.00648955, -0.0126788 ,  0.0210303 ,\n",
       "        0.0957282 , -0.0613672 , -0.0333282 , -0.0210865 , -0.0843227 ,\n",
       "        0.0578791 ,  0.052726  , -0.0566956 , -0.0407154 ,  0.0603111 ,\n",
       "       -0.0569204 , -0.0586587 , -0.0135658 ,  0.0254326 , -0.0944752 ,\n",
       "       -0.07885   , -0.00470421,  0.0174975 ,  0.0462485 ,  0.0642261 ,\n",
       "        0.0310251 ,  0.102567  , -0.041987  ,  0.0700113 , -0.0171799 ,\n",
       "        0.0922816 ,  0.00777218, -0.0109217 , -0.090305  , -0.0368232 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[words2ids[\"Słodkawy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence as seq\n",
    "def load_and_transform_data_to_phrases(labels, parents, tokens, words2ids):\n",
    "\n",
    "    \"\"\"\n",
    "    Dokumentacja\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    transform_label = {'-1':0, '0':1, '1':2}\n",
    "    \n",
    "    l = open(labels, \"r\", encoding=\"utf8\")\n",
    "    labels = [[transform_label[y] for y in x.split()] for x in l.readlines()] \n",
    "    l.close()\n",
    "\n",
    "    p = open(parents,\"r\", encoding=\"utf8\")\n",
    "    parents = [[int(y) for y in x.split()] for x in p.readlines()]\n",
    "    p.close()\n",
    "\n",
    "    t = open(tokens,\"r\", encoding=\"utf8\")\n",
    "    tokens = [x.split() for x in t.readlines()]\n",
    "    t.close()\n",
    "    \n",
    "    k = 0\n",
    "    result = []\n",
    "    \n",
    "    for labels_i,parents_i,tokens_i in zip(labels,parents,tokens):\n",
    "        \n",
    "        k = k + 1\n",
    "         \n",
    "        s = []\n",
    "        for i in range(len(tokens_i)):\n",
    "            s.append([i,int(parents_i[i]),labels_i[i],tokens_i[i]])\n",
    "\n",
    "\n",
    "        if len(s) == 1: #przypadek gdy fraza sklada sie z jednego tokena\n",
    "\n",
    "            result.append((\\\n",
    "                                  tokens[0],\n",
    "                                  np.array([words2ids.get(tokens[0], 1)]),\\\n",
    "                                  np.array(labels_i[0]) \\\n",
    "                              ))    \n",
    "                           \n",
    "        else: \n",
    "            \n",
    "            for i in range(len(s)): \n",
    "                children = []\n",
    "                for j in range(len(s)):\n",
    "                    if s[j][1] == i+1:\n",
    "                        children.append(s[j][0])\n",
    "                s[i].append(children)\n",
    "\n",
    "                \n",
    "            words = [x[0] for x in s]\n",
    "            children = [x[4] for x in s]\n",
    "            tokens = [x[3] for x in s]\n",
    "            labels_in_batch = [x[2] for x in s]\n",
    "        \n",
    "            phrases = [[k] for k in range(len(children))]\n",
    "            for i in range(len(children)):\n",
    "                for e in phrases[i]:\n",
    "                    phrases[i].extend(children[e])\n",
    "           \n",
    "            phrases = [ np.sort(x) for x in phrases]\n",
    "          \n",
    "            phrases = list(zip([np.array(tokens_i)[x] for x in phrases],\n",
    "                               [np.array([words2ids.get(t,1) for t in tokens_i])[x] for x in phrases],\n",
    "                               labels_i))\n",
    "\n",
    "            result.extend(phrases)\n",
    "           \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_and_transform_data_to_phrases(\"Dane/data_poleval/training-treebank/rev_labels.txt\", \"Dane/data_poleval/training-treebank/rev_parents.txt\",\"Dane/data_poleval/training-treebank/rev_sentence.txt\",words2ids)\n",
    "test_data = load_and_transform_data_to_phrases(\"Dane/data_poleval/poleval_test/gold_labels\", \"Dane/data_poleval/poleval_test/polevaltest_parents.txt\",\"Dane/data_poleval/poleval_test/polevaltest_sentence.txt\",words2ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Accuracy: %f' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_1.history['categorical_accuracy'], label = \"tarina\")\n",
    "plt.plot(history_1.history['val_categorical_accuracy'], label = \"test\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "Użyj Pretrain embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: %f' % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_1.history['categorical_accuracy'], label = \"tarina Trainable\")\n",
    "plt.plot(history_1.history['val_categorical_accuracy'], label = \"test Trainable\")\n",
    "\n",
    "plt.plot(history_2.history['categorical_accuracy'], label = \"tarina Not Trainable\")\n",
    "plt.plot(history_2.history['val_categorical_accuracy'], label = \"test Not Trainable\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "Użyj architektury\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 300,\n",
    "                   input_length=max_len,\n",
    "                  weights=[embeddings],\n",
    "                  trainable=False))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "Użyj architektury\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 300,\n",
    "                   input_length=max_len,\n",
    "                  weights=[embeddings],\n",
    "                  trainable=False))\n",
    "model.add(Conv1D(50,3))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "Co dokładnie robi \n",
    "\n",
    "```python\n",
    "from keras.layers.convolutional import Conv1D\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "Użyj architektury\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 300,\n",
    "                   input_length=max_len,\n",
    "                  weights=[embeddings],\n",
    "                  trainable=False))\n",
    "model.add(TimeDistributed(Dense(10,activation=\"tanh\")))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "Co dokładnie robi \n",
    "\n",
    "```python\n",
    "from keras.layers import TimeDistributed\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
