{
 "cells": [
  {
   "metadata": {
    "_uuid": "6b1ecd7c100ce998bf9b99e5f2e18a0ce7a6f960",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Topic will be convered in build the Machine Learning Model\n",
    "\n",
    "A) Introduction\n",
    "\n",
    "B) Load the data\n",
    "\n",
    "C) Filling missing Values\n",
    "\n",
    "D) Feature engineering\n",
    "\n",
    "E) Modeling\n",
    "\n",
    "F) Prediction"
   ]
  },
  {
   "metadata": {
    "_uuid": "b2f252b373d6105c9da4c262a7e28354d3824269",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "#### Introduction\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. That's why the name DieTanic. This is a very unforgetable disaster that no one in the world can forget.\n",
    "\n",
    "It took about $7.5 million to build the Titanic and it sunk under the ocean due to collision. The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle.\n",
    "\n",
    "The Objective of this notebook is to give an idea how is the workflow in any predictive modeling problem. How do we check features, how do we add new features and some Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it.\n",
    "\n",
    "If You Like the notebook and think that it helped you **PLEASE UPVOTE**. It will keep me motivated."
   ]
  },
  {
   "metadata": {
    "_uuid": "adcaca8b695db468e4c8d30d9b7cdbf372351cb8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Load the DataSet"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "fac6c9e0248f2e7c50809a609a003ca8c03bd028",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "badca40a17a7d6cca865d2eeb410b7b6372b2a04",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df=pd.read_csv(\"../input/train.csv\")\n",
    "test_df=pd.read_csv(\"../input/test.csv\")"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12084/3654507063.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrain_df\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"../input/train.csv\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mtest_df\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"../input/test.csv\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\podstawy_sztucznej_inteligencji\\lib\\site-packages\\pandas\\util\\_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m                 )\n\u001B[1;32m--> 311\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\podstawy_sztucznej_inteligencji\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    678\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    679\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 680\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    681\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    682\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\podstawy_sztucznej_inteligencji\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    573\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    574\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 575\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    576\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    577\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\podstawy_sztucznej_inteligencji\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    931\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    932\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[1;33m|\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 933\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    934\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    935\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\podstawy_sztucznej_inteligencji\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1215\u001B[0m             \u001B[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1216\u001B[0m             \u001B[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1217\u001B[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001B[0m\u001B[0;32m   1218\u001B[0m                 \u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1219\u001B[0m                 \u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\podstawy_sztucznej_inteligencji\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    787\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;34m\"b\"\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    788\u001B[0m             \u001B[1;31m# Encoding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 789\u001B[1;33m             handle = open(\n\u001B[0m\u001B[0;32m    790\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    791\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../input/train.csv'"
     ]
    }
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "eaa85d87d0ba5b30657b08be2f9150523fa16434",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "458ed3ace446f1619b7a0c5b89ea2776d767c4d7",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print('__Test_DataSet_')\n",
    "test_df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "e4ecd767783b19d65d1f2bc9bf166c169b71d617",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "The data has been split into two groups:\n",
    "\n",
    "training set (train.csv)\n",
    "test set (test.csv)\n",
    "The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n",
    "\n",
    "The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n",
    "\n",
    "\n",
    "### Data Dictionary\n",
    "Variable\tDefinition\tKey\n",
    "\n",
    "survival\tSurvival\t0 = No, 1 = Yes\n",
    "\n",
    "pclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n",
    "\n",
    "sex\tMale or Female\n",
    "\n",
    "Age\tAge in years\t\n",
    "\n",
    "sibsp\t# of siblings / spouses aboard the Titanic\t\n",
    "\n",
    "parch\t# of parents / children aboard the Titanic\t\n",
    "\n",
    "ticket\tTicket number\t\n",
    "\n",
    "fare\tPassenger fare\t\n",
    "\n",
    "cabin\tCabin number\t\n",
    "\n",
    "embarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n",
    "\n",
    "Variable Notes\n",
    "pclass: A proxy for socio-economic status (SES)\n",
    "1st = Upper\n",
    "2nd = Middle\n",
    "3rd = Lower\n",
    "\n",
    "age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    "sibsp: The dataset defines family relations in this way...\n",
    "\n",
    "Sibling = brother, sister, stepbrother, stepsister\n",
    "\n",
    "Spouse = husband, wife (mistresses and fiancés were ignored)\n",
    "\n",
    "parch: The dataset defines family relations in this way...\n",
    "\n",
    "Parent = mother, father\n",
    "\n",
    "Child = daughter, son, stepdaughter, stepson\n",
    "\n",
    "Some children travelled only with a nanny, therefore parch=0 for them"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "3da5936323af025e59ef5ba9405f11c94462f906",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "def missingdata(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    ms=pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    ms= ms[ms[\"Percent\"] > 0]\n",
    "    f,ax =plt.subplots(figsize=(8,6))\n",
    "    plt.xticks(rotation='90')\n",
    "    fig=sns.barplot(ms.index, ms[\"Percent\"],color=\"green\",alpha=0.8)\n",
    "    plt.xlabel('Features', fontsize=15)\n",
    "    plt.ylabel('Percent of missing values', fontsize=15)\n",
    "    plt.title('Percent missing data by feature', fontsize=15)\n",
    "    return ms"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "3d7cb33b3273a52fa069e9d5accdc5b3f0f2c0cb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Identifying Missing Value "
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "06d9b89abf88b8e2a8e65b7b5808e9b78ac318fb",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "missingdata(train_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "1ed3a5ffea751d569e3251f15482a9281b84b93d",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "missingdata(test_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "273008084c8fc87200ffae336720367d189db1ce",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Filling missing Values"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "4075789a9fe308c70efc8b25a366b1ea227ee677",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df['Age'].mean()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "3a0d4446b889bf83e946e75a91b9b5a43c8855ca",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "ce100b7b888b4382ffab8d8bf6a6f8700c458760",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df['Fare'].fillna(test_df['Fare'].median(), inplace = True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "fa0cc05930b32009befbacf47fc290d2662ea85a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Cabin Featueres has more than 75% of missing data in both Test and train data so we are remove the Cabin "
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "81634f45cebd3d1c4bc3c90c097d92b4197eb71f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "drop_column = ['Cabin']\n",
    "train_df.drop(drop_column, axis=1, inplace = True)\n",
    "test_df.drop(drop_column,axis=1,inplace=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "3cb02201ca58a20b6af2d83f8fa2b023dda61c1c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Both the test and train Age features contains more the 15% of missing Data so we are fill with the median"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "b1a2621a811f4b72c8eb22cc690592b50dd85376",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "test_df['Age'].fillna(test_df['Age'].median(), inplace = True)\n",
    "train_df['Age'].fillna(train_df['Age'].median(), inplace = True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "35ee380bab208f6fcd9115a335ebba25f49eaeec",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "print('check the nan value in train data')\n",
    "print(train_df.isnull().sum())\n",
    "print('___'*30)\n",
    "print('check the nan value in test data')\n",
    "print(test_df.isnull().sum())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "d1a25f32d3e237e17aec5ba90690bb918ed2f42a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Feature engineering\n",
    "\n",
    "Feature engineering is the art of converting raw data into useful features. There are several feature engineering techniques that you can apply to be an artist. A comprehensive list of them is presented by Heaton (2016). We will use just two techniques:\n",
    "\n",
    "Box-Cox transformations (Box & Cox 1964)\n",
    "\n",
    "Polynomials generation through non-linear expansions.\n",
    "Before the application of these techniques, we will just make some adjustments to the data, in order to prepare it for the modelling process."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "c56230fd340ccef0149e1e527940bef669118d3b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "## combine test and train as single to apply some function\n",
    "all_data=[train_df,test_df]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "751fd32d47a19698f06c167b69abb3712719c5ae",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "for dataset in all_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "53a26739acb4acc2a4daf48d19a5fa65b3e7a726",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "# Create a new feature Title, containing the titles of passenger names\n",
    "for dataset in all_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "for dataset in all_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', \n",
    "                                                 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "f064a845106072249cb0cda674622ba624cef9a8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "## create bin for age features\n",
    "for dataset in all_data:\n",
    "    dataset['Age_bin'] = pd.cut(dataset['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "0293d8cc8f0e61c7b9cdc48132b9e1f967ff12a3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "## create bin for fare features\n",
    "for dataset in all_data:\n",
    "    dataset['Fare_bin'] = pd.cut(dataset['Fare'], bins=[0,7.91,14.45,31,120], labels=['Low_fare','median_fare',\n",
    "                                                                                      'Average_fare','high_fare'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "b7d4ec047af189e73c055a720547e9a743a63d3b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "### for our reference making a copy of both DataSet start working for copy of dataset\n",
    "traindf=train_df\n",
    "testdf=test_df"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "0b76607e2cc5474404de02902f9a55dc1f4dbe4b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "all_dat=[traindf,testdf]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "cadab8ab7be3e631b62a7c6b3a97404226147097",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "for dataset in all_dat:\n",
    "    drop_column = ['Age','Fare','Name','Ticket']\n",
    "    dataset.drop(drop_column, axis=1, inplace = True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "1927c8d03347df6a12f6098557ad7e3c9eb9c3de",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "drop_column = ['PassengerId']\n",
    "traindf.drop(drop_column, axis=1, inplace = True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "3129e64ff1587642e87836dbc806ec12b27802ec",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### now every thing almost ready only one step we converted the catergical features in numerical by using dummy variable"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "47a187a0b86674831e752f62f368492e74f5b140",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "testdf.head(2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "dd15379c46c6ff0e3738779e920920cad43bd363",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "traindf = pd.get_dummies(traindf, columns = [\"Sex\",\"Title\",\"Age_bin\",\"Embarked\",\"Fare_bin\"],\n",
    "                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "collapsed": true,
    "_uuid": "339ae785a3bc4cdb3f94c032c4576df600b21af0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "testdf = pd.get_dummies(testdf, columns = [\"Sex\",\"Title\",\"Age_bin\",\"Embarked\",\"Fare_bin\"],\n",
    "                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "9e7feb8f222da43f54a9a375f8f14388b34a9034",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "testdf.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "ce4d8da240bc95139a5e3e5df16ca0d8fa89661f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Correlation Between The Features"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "1d58eb8191b02540aa33f273709e5d98566cc5ff",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "sns.heatmap(traindf.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(20,12)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "5fb3af5ae491cd5f330d92ac5f2026e697ef004d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Interpreting The Heatmap\n",
    "The first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n",
    "\n",
    "POSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n",
    "\n",
    "NEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n",
    "\n",
    "Now lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n",
    "\n",
    "So do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n",
    "\n",
    "Now from the above heatmap,we can see that the features are not much correlated. The highest correlation is between SibSp and Parch i.e 0.41. So we can carry on with all features."
   ]
  },
  {
   "metadata": {
    "_uuid": "e15e03088a501b45135dcf86bfb0b44cf2585cb9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Pairplots\n",
    "Finally let us generate some pairplots to observe the distribution of data from one feature to the other. Once again we use Seaborn to help us."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "c6d8bf1cccd3d37ef7d016d456bfd71b2ab240e8",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "g = sns.pairplot(data=train_df, hue='Survived', palette = 'seismic',\n",
    "                 size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\n",
    "g.set(xticklabels=[])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "528fab46604964ba44f6c5753864795950cecd10",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "###  Model\n",
    "Now we are ready to train a model and predict the required solution. There are lot of predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "KNN\n",
    "\n",
    "Support Vector Machines\n",
    "\n",
    "Naive Bayes classifier\n",
    "\n",
    "Decision Tree\n",
    "\n",
    "Random Forrest\n",
    "\n",
    "Linear Discriminant Analysis\n",
    "\n",
    "Ada Boost Classifier \n",
    "\n",
    "Gradient Boosting Classifier\n",
    "\n",
    "And also compared above given classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure and plot accuracy based confusion matrix"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "f959b89460e4d08d56725083ecc6855bcaddd61a",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split #for split the data\n",
    "from sklearn.metrics import accuracy_score  #for accuracy_score\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "all_features = traindf.drop(\"Survived\",axis=1)\n",
    "Targeted_feature = traindf[\"Survived\"]\n",
    "X_train,X_test,y_train,y_test = train_test_split(all_features,Targeted_feature,test_size=0.3,random_state=42)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "9b72dbbc9a2f2138e73b56578479dd85990fb5cb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "5f1f674504b6598ed928437b7582db275bf5ac62",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression # Logistic Regression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "prediction_lr=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the Logistic Regression is',round(accuracy_score(prediction_lr,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_lr=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for Logistic REgression is:',round(result_lr.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "cf9e72500c2c57a4133d95955352eb016efad5cb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Random Forest Classifier "
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "cab128ec78b6666517eaccd4eea9c0ecdda5b2dd",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(criterion='gini', n_estimators=700,\n",
    "                             min_samples_split=10,min_samples_leaf=1,\n",
    "                             max_features='auto',oob_score=True,\n",
    "                             random_state=1,n_jobs=-1)\n",
    "model.fit(X_train,y_train)\n",
    "prediction_rm=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the Random Forest Classifier is',round(accuracy_score(prediction_rm,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_rm=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for Random Forest Classifier is:',round(result_rm.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "e8b80047616dcf8429dd912e2fc899727b5569b4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "4ec3da5a602397b8cd15ff9bed604bccf19bbfbe",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# Support Vector Machines\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train,y_train)\n",
    "prediction_svm=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the Support Vector Machines Classifier is',round(accuracy_score(prediction_svm,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_svm=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for Support Vector Machines Classifier is:',round(result_svm.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "52d0b969fb3e3a610c08eedaf67217a876fd33c9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "be01ce01c4529a6041ee43948f72c0ed0d48eeea",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "##knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors = 4)\n",
    "model.fit(X_train,y_train)\n",
    "prediction_knn=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the K Nearst Neighbors Classifier is',round(accuracy_score(prediction_knn,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_knn=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for K Nearest Neighbors Classifier is:',round(result_knn.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "0c174c7d7447fc6ccbf2510f86797b005fe8027d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "196a9d560e45312ba79b5bf50c598e6bb6bdc182",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model= GaussianNB()\n",
    "model.fit(X_train,y_train)\n",
    "prediction_gnb=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the Gaussian Naive Bayes Classifier is',round(accuracy_score(prediction_gnb,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_gnb=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for Gaussian Naive Bayes classifier is:',round(result_gnb.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "40ae46c9272d37f1a61db5718472d32cdb1b4c07",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model= DecisionTreeClassifier(criterion='gini', \n",
    "                             min_samples_split=10,min_samples_leaf=1,\n",
    "                             max_features='auto')\n",
    "model.fit(X_train,y_train)\n",
    "prediction_tree=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the DecisionTree Classifier is',round(accuracy_score(prediction_tree,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_tree=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for Decision Tree classifier is:',round(result_tree.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "43e42f617613bb5f4a74664532b77be88f0d3304",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "47684fb14c7704a30f09b4e34ce64bece077144b",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model= AdaBoostClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "prediction_adb=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the AdaBoostClassifier is',round(accuracy_score(prediction_adb,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_adb=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for AdaBoostClassifier is:',round(result_adb.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "a984340a9396859723a5af60c5dd94ac8a9ef1ed",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "aa8651a7b69a739751ad695b2d2873be41350b79",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model= LinearDiscriminantAnalysis()\n",
    "model.fit(X_train,y_train)\n",
    "prediction_lda=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the LinearDiscriminantAnalysis is',round(accuracy_score(prediction_lda,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_lda=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for AdaBoostClassifier is:',round(result_lda.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "04947698ee4b2c872fdfe83e37752130a87cc53f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "055a04e27abc02803fbe4137464345756952eaff",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model= GradientBoostingClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "prediction_gbc=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the Gradient Boosting Classifier is',round(accuracy_score(prediction_gbc,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_gbc=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for AdaBoostClassifier is:',round(result_gbc.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "b26b42dffb8b3c587a10ed60d3346176621a9f49",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Model evaluation\n",
    "We can now rank our evaluation of all the models to choose the best one for our problem."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "de0c942f0f7359dc1b47a18897fcceff996e7a4c",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes', 'AdaBoostClassifier', \n",
    "              'Gradient Decent', 'Linear Discriminant Analysis', \n",
    "              'Decision Tree'],\n",
    "    'Score': [result_svm.mean(), result_knn.mean(), result_lr.mean(), \n",
    "              result_rm.mean(), result_gnb.mean(), result_adb.mean(), \n",
    "              result_gbc.mean(), result_lda.mean(), result_tree.mean()]})\n",
    "models.sort_values(by='Score',ascending=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "6c62a4ad14ef02f9f3e6bc82fa766e46f61dde1a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "By looking at all the matrices, we can say that Random Forest & SVM  classifier has a higher chance in correctly predicting dead passengers."
   ]
  },
  {
   "metadata": {
    "_uuid": "a37cf22f7ef85a686cd48e9cb4b2582f526c686c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Hyper-Parameters Tuning\n",
    "\n",
    "The machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning\n",
    "\n",
    "So based on the above given  acuracy result i will performance Grid search and random search for the \n",
    "SVM \n",
    "\n",
    "LDA\n",
    "\n",
    "Logistic Regression \n",
    "\n",
    "Gradient Decent Classifier\n",
    "\n",
    "Random Forest Classifier\n",
    "\n",
    "Parameters\n",
    "\n",
    "Just a quick summary of the parameters that we will be listing here for completeness,\n",
    "\n",
    "n_jobs : Number of cores used for the training process. If set to -1, all cores are used.\n",
    "\n",
    "n_estimators : Number of classification trees in your learning model ( set to 10 per default)\n",
    "\n",
    "max_depth : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high a number would run the risk of overfitting as one would be growing the tree too deep\n",
    "\n",
    "verbose : Controls whether you want to output any text during the learning process. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration.\n",
    "\n",
    "Please check out the full description via the official Sklearn website. There you will find that there are a whole host of other useful parameters that you can play around with."
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "ac038af9e3e41ac55ab3c6a1f3fb9dc057b380f9",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "train_X = traindf.drop(\"Survived\", axis=1)\n",
    "train_Y=traindf[\"Survived\"]\n",
    "test_X  = testdf.drop(\"PassengerId\", axis=1).copy()\n",
    "train_X.shape, train_Y.shape, test_X.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "2ed60395f5cb74c712f2cf5db92e32664c371415",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gradient boosting tunning\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "param_grid = {'loss' : [\"deviance\"],\n",
    "              'n_estimators' : [100,200,300,400],\n",
    "              'learning_rate': [0.1, 0.05, 0.01,0.001],\n",
    "              'max_depth': [4, 8],\n",
    "              'min_samples_leaf': [100,150],\n",
    "              'max_features': [0.3, 0.2,0.1] \n",
    "              }\n",
    "\n",
    "modelf = GridSearchCV(model,param_grid = param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "modelf.fit(train_X,train_Y)\n",
    "\n",
    "# Best score\n",
    "modelf.best_score_\n",
    "\n",
    "# Best Estimator\n",
    "modelf.best_estimator_"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "c49a04e4786bfa205336e36d0d924b0db6955652",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "modelf.best_score_"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "b829b91d12a4a6e9fff281ce31a9f2a3f817ba3c",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# Random Forest Classifier Parameters tunning \n",
    "model = RandomForestClassifier()\n",
    "n_estim=range(100,1000,100)\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "param_grid = {\"n_estimators\" :n_estim}\n",
    "\n",
    "\n",
    "model_rf = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "model_rf.fit(train_X,train_Y)\n",
    "\n",
    "\n",
    "\n",
    "# Best score\n",
    "print(model_rf.best_score_)\n",
    "\n",
    "#best estimator\n",
    "model_rf.best_estimator_"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "a7b18145e57af1a3f801dcbcf3d9cd843e868c8e",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model =LinearDiscriminantAnalysis()\n",
    "param_grid = {'tol':[0.001,0.01,.1,.2]}\n",
    "\n",
    "modell = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "modell.fit(train_X,train_Y)\n",
    "\n",
    "# Best score\n",
    "print(modell.best_score_)\n",
    "\n",
    "# Best Estimator\n",
    "modell.best_estimator_\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "4bd25f50af363bfdb9fb94eb9deb07973fb68d6a",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "model= SVC()\n",
    "param_grid = {'kernel': ['rbf','linear'], \n",
    "                  'gamma': [ 0.001, 0.01, 0.1, 1],\n",
    "                  'C': [1, 10, 50, 100,200,300, 1000]}\n",
    "\n",
    "modelsvm = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "modelsvm.fit(train_X,train_Y)\n",
    "\n",
    "print(modelsvm.best_estimator_)\n",
    "\n",
    "# Best score\n",
    "print(modelsvm.best_score_)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "6743e4c565b19c5dd405dd919077c327c62ef407",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Apply the Estimator which got from parameter tuning of Random Forest "
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "72e681c98a708b22ba49682d5c218aa4c723db76",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "# Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "random_forest.fit(train_X, train_Y)\n",
    "Y_pred_rf = random_forest.predict(test_X)\n",
    "random_forest.score(train_X,train_Y)\n",
    "acc_random_forest = round(random_forest.score(train_X, train_Y) * 100, 2)\n",
    "\n",
    "print(\"Important features\")\n",
    "pd.Series(random_forest.feature_importances_,train_X.columns).sort_values(ascending=True).plot.barh(width=0.8)\n",
    "print('__'*30)\n",
    "print(acc_random_forest)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "_uuid": "6bdd506893dafe4a23aba31dc91d4dd8f0f2281f",
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": Y_pred_rf})"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "f70d6ee14de6b8446d051cb5e836ecbe0400edb7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "###  result"
   ]
  },
  {
   "metadata": {
    "_uuid": "b964537b1b8fc71aa729984ff47515c34122fc82",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "those are  work for buliding ML model which gives best result  i have done please vote for me which help my movitvation to increase to do a lot of work if there any imporvment can be done means please say in comments References This notebook has been created based on great work done solving the Titanic competition and other sources."
   ]
  },
  {
   "metadata": {
    "trusted": false,
    "collapsed": true,
    "_uuid": "431ca436cf44ffa34a5e55939231414552ba2b37",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}