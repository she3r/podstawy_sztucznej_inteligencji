{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "print(tf.__version__)\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, SimpleRNN, LSTM, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import History\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "It must specify 3 arguments:\n",
    "\n",
    "* **input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "* **output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "* **input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "\n",
    "# Zad. \n",
    "Podążamy za stroną: \n",
    "\n",
    "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "mamy jakiś zbiór tekstów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "    'Good work',\n",
    "    'Great effort',\n",
    "    'nice work',\n",
    "    'Excellent!',\n",
    "    'Weak',\n",
    "    'Poor effort!',\n",
    "    'not good',\n",
    "    'poor work',\n",
    "    'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do warstwy **Embedding layer** wchodzi sekwencja intów.\n",
    "\n",
    "* my wykorzystamy reprezenatację Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22, 3], [43, 37], [33, 20], [46, 37], [43], [5], [34, 20], [8, 43], [34, 37], [27, 12, 3, 45]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekwencje mają różne długości, a Keras wymaga aby wejścia były równej długość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22  3  0  0]\n",
      " [43 37  0  0]\n",
      " [33 20  0  0]\n",
      " [46 37  0  0]\n",
      " [43  0  0  0]\n",
      " [ 5  0  0  0]\n",
      " [34 20  0  0]\n",
      " [ 8 43  0  0]\n",
      " [34 37  0  0]\n",
      " [27 12  3 45]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embeding ma zakres 50 i długość wejściową 4. Zmniejszmy embending do wymiaru 8.\n",
    "* Model jest prostym klasyfikatorem binarnym. \n",
    "* Co ważne, wynik z warstwy Embeding będzie wynosił 4 wektory o 8 wymiarach każdy, po jednym dla każdego słowa. \n",
    "* Spłaszczamy to do jednego 32-elementowego wektora, aby przejść do warstwy wyjściowej Dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "history_1 = History()\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.6928 - accuracy: 0.5556 - val_loss: 0.6744 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6917 - accuracy: 0.5556 - val_loss: 0.6748 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6907 - accuracy: 0.5556 - val_loss: 0.6752 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6896 - accuracy: 0.5556 - val_loss: 0.6756 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6885 - accuracy: 0.5556 - val_loss: 0.6759 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6875 - accuracy: 0.5556 - val_loss: 0.6763 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6865 - accuracy: 0.5556 - val_loss: 0.6766 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6854 - accuracy: 0.5556 - val_loss: 0.6770 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6844 - accuracy: 0.5556 - val_loss: 0.6773 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6834 - accuracy: 0.5556 - val_loss: 0.6777 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6823 - accuracy: 0.5556 - val_loss: 0.6780 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6813 - accuracy: 0.5556 - val_loss: 0.6783 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6803 - accuracy: 0.5556 - val_loss: 0.6786 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6793 - accuracy: 0.5556 - val_loss: 0.6789 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6783 - accuracy: 0.5556 - val_loss: 0.6792 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6772 - accuracy: 0.5556 - val_loss: 0.6795 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6762 - accuracy: 0.5556 - val_loss: 0.6798 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6752 - accuracy: 0.5556 - val_loss: 0.6800 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6742 - accuracy: 0.5556 - val_loss: 0.6803 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6731 - accuracy: 0.5556 - val_loss: 0.6805 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6721 - accuracy: 0.5556 - val_loss: 0.6808 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6711 - accuracy: 0.5556 - val_loss: 0.6810 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6700 - accuracy: 0.5556 - val_loss: 0.6812 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6690 - accuracy: 0.5556 - val_loss: 0.6814 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6679 - accuracy: 0.5556 - val_loss: 0.6815 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6669 - accuracy: 0.5556 - val_loss: 0.6817 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6658 - accuracy: 0.5556 - val_loss: 0.6819 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6647 - accuracy: 0.5556 - val_loss: 0.6820 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6636 - accuracy: 0.5556 - val_loss: 0.6821 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6626 - accuracy: 0.5556 - val_loss: 0.6823 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6615 - accuracy: 0.5556 - val_loss: 0.6824 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6604 - accuracy: 0.5556 - val_loss: 0.6825 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6593 - accuracy: 0.5556 - val_loss: 0.6825 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6581 - accuracy: 0.5556 - val_loss: 0.6826 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6570 - accuracy: 0.5556 - val_loss: 0.6827 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6559 - accuracy: 0.5556 - val_loss: 0.6827 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6547 - accuracy: 0.5556 - val_loss: 0.6827 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6536 - accuracy: 0.5556 - val_loss: 0.6827 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6524 - accuracy: 0.5556 - val_loss: 0.6827 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6513 - accuracy: 0.5556 - val_loss: 0.6827 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6501 - accuracy: 0.5556 - val_loss: 0.6827 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6489 - accuracy: 0.5556 - val_loss: 0.6827 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6477 - accuracy: 0.5556 - val_loss: 0.6826 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6465 - accuracy: 0.5556 - val_loss: 0.6826 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6453 - accuracy: 0.5556 - val_loss: 0.6825 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6440 - accuracy: 0.5556 - val_loss: 0.6824 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6428 - accuracy: 0.5556 - val_loss: 0.6823 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6415 - accuracy: 0.5556 - val_loss: 0.6822 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6403 - accuracy: 0.5556 - val_loss: 0.6821 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6390 - accuracy: 0.5556 - val_loss: 0.6820 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6377 - accuracy: 0.5556 - val_loss: 0.6818 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6364 - accuracy: 0.5556 - val_loss: 0.6817 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6351 - accuracy: 0.5556 - val_loss: 0.6815 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6338 - accuracy: 0.5556 - val_loss: 0.6814 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6325 - accuracy: 0.5556 - val_loss: 0.6812 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6312 - accuracy: 0.5556 - val_loss: 0.6810 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6298 - accuracy: 0.6667 - val_loss: 0.6808 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6285 - accuracy: 0.6667 - val_loss: 0.6806 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6271 - accuracy: 0.7778 - val_loss: 0.6804 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6257 - accuracy: 0.7778 - val_loss: 0.6802 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6243 - accuracy: 0.7778 - val_loss: 0.6800 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6229 - accuracy: 0.7778 - val_loss: 0.6798 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6215 - accuracy: 0.7778 - val_loss: 0.6795 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6201 - accuracy: 0.7778 - val_loss: 0.6793 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6186 - accuracy: 0.7778 - val_loss: 0.6790 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6172 - accuracy: 0.7778 - val_loss: 0.6788 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6157 - accuracy: 0.7778 - val_loss: 0.6785 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6143 - accuracy: 0.7778 - val_loss: 0.6783 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6128 - accuracy: 0.7778 - val_loss: 0.6780 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6113 - accuracy: 0.7778 - val_loss: 0.6778 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6098 - accuracy: 0.7778 - val_loss: 0.6775 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6083 - accuracy: 0.7778 - val_loss: 0.6773 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6067 - accuracy: 0.8889 - val_loss: 0.6770 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6052 - accuracy: 0.8889 - val_loss: 0.6767 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6036 - accuracy: 0.8889 - val_loss: 0.6765 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6021 - accuracy: 0.8889 - val_loss: 0.6762 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6005 - accuracy: 0.8889 - val_loss: 0.6759 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5989 - accuracy: 0.8889 - val_loss: 0.6757 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5973 - accuracy: 0.8889 - val_loss: 0.6754 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5957 - accuracy: 0.8889 - val_loss: 0.6751 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5941 - accuracy: 0.8889 - val_loss: 0.6749 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5925 - accuracy: 0.8889 - val_loss: 0.6746 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5908 - accuracy: 0.8889 - val_loss: 0.6743 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5892 - accuracy: 0.8889 - val_loss: 0.6741 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5875 - accuracy: 0.8889 - val_loss: 0.6738 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5859 - accuracy: 0.8889 - val_loss: 0.6736 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5842 - accuracy: 0.8889 - val_loss: 0.6733 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5825 - accuracy: 0.8889 - val_loss: 0.6731 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5808 - accuracy: 0.8889 - val_loss: 0.6728 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5791 - accuracy: 0.8889 - val_loss: 0.6726 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5774 - accuracy: 0.8889 - val_loss: 0.6723 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5756 - accuracy: 0.8889 - val_loss: 0.6721 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5739 - accuracy: 0.8889 - val_loss: 0.6718 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5721 - accuracy: 0.8889 - val_loss: 0.6716 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5704 - accuracy: 0.8889 - val_loss: 0.6713 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5686 - accuracy: 0.8889 - val_loss: 0.6711 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5668 - accuracy: 0.8889 - val_loss: 0.6709 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5651 - accuracy: 0.8889 - val_loss: 0.6706 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5633 - accuracy: 0.8889 - val_loss: 0.6704 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5615 - accuracy: 0.8889 - val_loss: 0.6702 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x47a4446bc8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # compile the model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# # fit the model\n",
    "# model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "# model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(padded_docs, labels, epochs=100, validation_split=0.1, callbacks=[history_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_1.history['accuracy'], label = \"tarina Adam\")\n",
    "plt.plot(history_1.history['val_accuracy'], label = \"test Adam\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain embedding\n",
    "\n",
    "https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "* GloVe embedding data can be found at: http://nlp.stanford.edu/data/glove.6B.zip (source page: http://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "* After downloading and unzipping, you will see a few files, one of which is “glove.6B.50d.txt“, which contains a 100-dimensional version of the embedding.\n",
    "\n",
    "\n",
    "Pojedyńczy plik można pobrać z tąd:\n",
    "https://www.dropbox.com/sh/tjq47ybybgnrbel/AAAVbp0UkQTAbKWVMIi5mtHpa?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B.50d')\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "# file = open(filename, encoding=\"utf8\")\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'), encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11008  , -0.38781  , -0.57615  , -0.27714  ,  0.70521  ,\n",
       "        0.53994  , -1.0786   , -0.40146  ,  1.1504   , -0.5678   ,\n",
       "        0.0038977,  0.52878  ,  0.64561  ,  0.47262  ,  0.48549  ,\n",
       "       -0.18407  ,  0.1801   ,  0.91397  , -1.1979   , -0.5778   ,\n",
       "       -0.37985  ,  0.33606  ,  0.772    ,  0.75555  ,  0.45506  ,\n",
       "       -1.7671   , -1.0503   ,  0.42566  ,  0.41893  , -0.68327  ,\n",
       "        1.5673   ,  0.27685  , -0.61708  ,  0.64638  , -0.076996 ,\n",
       "        0.37118  ,  0.1308   , -0.45137  ,  0.25398  , -0.74392  ,\n",
       "       -0.086199 ,  0.24068  , -0.64819  ,  0.83549  ,  1.2502   ,\n",
       "       -0.51379  ,  0.04224  , -0.88118  ,  0.7158   ,  0.38519  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = embeddings_index[\"king\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = embeddings_index[\"queen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = embeddings_index[\"prince\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s4 = embeddings_index[\"house\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king queen 0.2160956859588623\n",
      "king prince 0.17638200521469116\n",
      "king house 0.47720617055892944\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "print(\"king\", \"queen\", distance.cosine(s1, s2))\n",
    "print(\"king\", \"prince\", distance.cosine(s1, s3))\n",
    "print(\"king\", \"house\", distance.cosine(s1, s4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a Tokenizer class that can be fit on the training data, can convert text to sequences consistently by calling the texts_to_sequences() method on the Tokenizer class, and provides access to the dictionary mapping of words to integers in a word_index attribute.\n",
    "\n",
    "https://keras.io/preprocessing/text/#tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekwencje mają różne długości, a Keras wymaga aby wejścia były równej długość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight vector from the loaded GloVe embedding.\n",
    "\n",
    "The result is a matrix of weights only for words we will see during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 50)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix.shape        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.13589978e-01,  1.96950004e-01, -5.19439995e-01, -8.62179995e-01,\n",
       "        1.54940002e-02,  1.09729998e-01, -8.02929997e-01, -3.33609998e-01,\n",
       "       -1.61189993e-04,  1.01889996e-02,  4.67340015e-02,  4.67510015e-01,\n",
       "       -4.74750012e-01,  1.10380001e-01,  3.93269986e-01, -4.36520010e-01,\n",
       "        3.99839997e-01,  2.71090001e-01,  4.26499993e-01, -6.06400013e-01,\n",
       "        8.11450005e-01,  4.56299990e-01, -1.27260000e-01, -2.24739999e-01,\n",
       "        6.40709996e-01, -1.27670002e+00, -7.22310007e-01, -6.95900023e-01,\n",
       "        2.80450005e-02, -2.30719998e-01,  3.79959989e+00, -1.26249999e-01,\n",
       "       -4.79669988e-01, -9.99719977e-01, -2.19760001e-01,  5.05649984e-01,\n",
       "        2.59530004e-02,  8.05140018e-01,  1.99290007e-01,  2.87959993e-01,\n",
       "       -1.59150004e-01, -3.04380000e-01,  1.60249993e-01, -1.82899997e-01,\n",
       "       -3.85629982e-02, -1.76190004e-01,  2.70409994e-02,  4.68420014e-02,\n",
       "       -6.28970027e-01,  3.57259989e-01])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.13589978e-01,  1.96950004e-01, -5.19439995e-01, -8.62179995e-01,\n",
       "        1.54940002e-02,  1.09729998e-01, -8.02929997e-01, -3.33609998e-01,\n",
       "       -1.61189993e-04,  1.01889996e-02,  4.67340015e-02,  4.67510015e-01,\n",
       "       -4.74750012e-01,  1.10380001e-01,  3.93269986e-01, -4.36520010e-01,\n",
       "        3.99839997e-01,  2.71090001e-01,  4.26499993e-01, -6.06400013e-01,\n",
       "        8.11450005e-01,  4.56299990e-01, -1.27260000e-01, -2.24739999e-01,\n",
       "        6.40709996e-01, -1.27670002e+00, -7.22310007e-01, -6.95900023e-01,\n",
       "        2.80450005e-02, -2.30719998e-01,  3.79959989e+00, -1.26249999e-01,\n",
       "       -4.79669988e-01, -9.99719977e-01, -2.19760001e-01,  5.05649984e-01,\n",
       "        2.59530004e-02,  8.05140018e-01,  1.99290007e-01,  2.87959993e-01,\n",
       "       -1.59150004e-01, -3.04380000e-01,  1.60249993e-01, -1.82899997e-01,\n",
       "       -3.85629982e-02, -1.76190004e-01,  2.70409994e-02,  4.68420014e-02,\n",
       "       -6.28970027e-01,  3.57259989e-01])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference is that the embedding layer can be seeded with the GloVe word embedding weights. We chose the 50-dimensional version, therefore the Embedding layer must be defined with output_dim set to 50. Finally, we do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=4, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 4, 50)             750       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 951\n",
      "Trainable params: 201\n",
      "Non-trainable params: 750\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "history_2 = History()\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.7850 - accuracy: 0.4444 - val_loss: 0.4714 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7792 - accuracy: 0.4444 - val_loss: 0.4682 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7736 - accuracy: 0.4444 - val_loss: 0.4651 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7679 - accuracy: 0.4444 - val_loss: 0.4622 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7623 - accuracy: 0.4444 - val_loss: 0.4595 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7568 - accuracy: 0.4444 - val_loss: 0.4570 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7512 - accuracy: 0.4444 - val_loss: 0.4547 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7458 - accuracy: 0.4444 - val_loss: 0.4527 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7404 - accuracy: 0.4444 - val_loss: 0.4508 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7350 - accuracy: 0.4444 - val_loss: 0.4491 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7297 - accuracy: 0.6667 - val_loss: 0.4477 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7244 - accuracy: 0.6667 - val_loss: 0.4464 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7191 - accuracy: 0.6667 - val_loss: 0.4453 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7139 - accuracy: 0.6667 - val_loss: 0.4444 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7087 - accuracy: 0.6667 - val_loss: 0.4437 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7036 - accuracy: 0.6667 - val_loss: 0.4430 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6985 - accuracy: 0.6667 - val_loss: 0.4425 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6935 - accuracy: 0.6667 - val_loss: 0.4420 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6884 - accuracy: 0.6667 - val_loss: 0.4417 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6835 - accuracy: 0.6667 - val_loss: 0.4414 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6786 - accuracy: 0.7778 - val_loss: 0.4411 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6737 - accuracy: 0.7778 - val_loss: 0.4409 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6688 - accuracy: 0.7778 - val_loss: 0.4407 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6640 - accuracy: 0.7778 - val_loss: 0.4405 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6593 - accuracy: 0.7778 - val_loss: 0.4403 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6545 - accuracy: 0.7778 - val_loss: 0.4401 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6499 - accuracy: 0.7778 - val_loss: 0.4399 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6452 - accuracy: 0.7778 - val_loss: 0.4397 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6406 - accuracy: 0.7778 - val_loss: 0.4395 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6361 - accuracy: 0.7778 - val_loss: 0.4393 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6316 - accuracy: 0.7778 - val_loss: 0.4391 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6271 - accuracy: 0.7778 - val_loss: 0.4389 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6227 - accuracy: 0.7778 - val_loss: 0.4387 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6183 - accuracy: 0.7778 - val_loss: 0.4385 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6139 - accuracy: 0.7778 - val_loss: 0.4384 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6096 - accuracy: 0.7778 - val_loss: 0.4383 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6053 - accuracy: 0.7778 - val_loss: 0.4382 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6010 - accuracy: 0.7778 - val_loss: 0.4381 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5968 - accuracy: 0.7778 - val_loss: 0.4381 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5927 - accuracy: 0.7778 - val_loss: 0.4382 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5885 - accuracy: 0.7778 - val_loss: 0.4383 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5845 - accuracy: 0.7778 - val_loss: 0.4384 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5804 - accuracy: 0.7778 - val_loss: 0.4386 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5764 - accuracy: 0.7778 - val_loss: 0.4389 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5724 - accuracy: 0.7778 - val_loss: 0.4392 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5684 - accuracy: 0.7778 - val_loss: 0.4396 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5645 - accuracy: 0.7778 - val_loss: 0.4400 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5606 - accuracy: 0.7778 - val_loss: 0.4405 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5568 - accuracy: 0.7778 - val_loss: 0.4410 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5530 - accuracy: 0.8889 - val_loss: 0.4416 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5492 - accuracy: 0.8889 - val_loss: 0.4421 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5455 - accuracy: 0.8889 - val_loss: 0.4427 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5418 - accuracy: 0.8889 - val_loss: 0.4434 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5381 - accuracy: 0.8889 - val_loss: 0.4440 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5344 - accuracy: 0.8889 - val_loss: 0.4447 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5308 - accuracy: 0.8889 - val_loss: 0.4453 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5272 - accuracy: 0.8889 - val_loss: 0.4460 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5237 - accuracy: 0.8889 - val_loss: 0.4467 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5202 - accuracy: 0.8889 - val_loss: 0.4474 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5167 - accuracy: 0.8889 - val_loss: 0.4481 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5132 - accuracy: 0.8889 - val_loss: 0.4488 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5098 - accuracy: 0.8889 - val_loss: 0.4495 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5064 - accuracy: 0.8889 - val_loss: 0.4503 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5030 - accuracy: 0.8889 - val_loss: 0.4510 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4997 - accuracy: 0.8889 - val_loss: 0.4518 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4964 - accuracy: 0.8889 - val_loss: 0.4525 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4931 - accuracy: 0.8889 - val_loss: 0.4533 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4898 - accuracy: 0.8889 - val_loss: 0.4541 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4866 - accuracy: 0.8889 - val_loss: 0.4549 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4834 - accuracy: 0.8889 - val_loss: 0.4558 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4802 - accuracy: 0.8889 - val_loss: 0.4566 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4771 - accuracy: 0.8889 - val_loss: 0.4575 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4740 - accuracy: 0.8889 - val_loss: 0.4584 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4709 - accuracy: 0.8889 - val_loss: 0.4593 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4678 - accuracy: 0.8889 - val_loss: 0.4603 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4647 - accuracy: 0.8889 - val_loss: 0.4612 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4617 - accuracy: 0.8889 - val_loss: 0.4622 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4587 - accuracy: 0.8889 - val_loss: 0.4632 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4558 - accuracy: 0.8889 - val_loss: 0.4642 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4528 - accuracy: 0.8889 - val_loss: 0.4652 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4499 - accuracy: 0.8889 - val_loss: 0.4662 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4470 - accuracy: 0.8889 - val_loss: 0.4672 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4441 - accuracy: 0.8889 - val_loss: 0.4683 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4413 - accuracy: 0.8889 - val_loss: 0.4694 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4385 - accuracy: 0.8889 - val_loss: 0.4704 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4357 - accuracy: 0.8889 - val_loss: 0.4715 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4329 - accuracy: 0.8889 - val_loss: 0.4726 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4301 - accuracy: 0.8889 - val_loss: 0.4737 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4274 - accuracy: 0.8889 - val_loss: 0.4748 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4247 - accuracy: 0.8889 - val_loss: 0.4759 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4220 - accuracy: 0.8889 - val_loss: 0.4770 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4193 - accuracy: 0.8889 - val_loss: 0.4781 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4167 - accuracy: 0.8889 - val_loss: 0.4792 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4141 - accuracy: 0.8889 - val_loss: 0.4804 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4115 - accuracy: 0.8889 - val_loss: 0.4815 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4089 - accuracy: 0.8889 - val_loss: 0.4827 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4063 - accuracy: 0.8889 - val_loss: 0.4838 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4038 - accuracy: 0.8889 - val_loss: 0.4850 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4013 - accuracy: 0.8889 - val_loss: 0.4862 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3988 - accuracy: 1.0000 - val_loss: 0.4874 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xdf0bdd8f88>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(padded_docs, labels, epochs=100, validation_split=0.1, callbacks=[history_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_1.history['accuracy'], label = \"tarina Trainable\")\n",
    "plt.plot(history_1.history['val_accuracy'], label = \"test Trainable\")\n",
    "\n",
    "plt.plot(history_2.history['accuracy'], label = \"tarina Not Trainable\")\n",
    "plt.plot(history_2.history['val_accuracy'], label = \"test Not Trainable\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
